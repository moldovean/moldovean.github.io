<!DOCTYPE html>
<html lang="en">
  <head>
    <title> Hidden Markov Model Transition Matrix Estimation</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
    <!-- jQuery library -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.2/jquery.min.js"></script>
    <!-- Latest compiled JavaScript -->
    <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
    <!-- MathJax -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  </head>

  <body>
    <div class="container">
      <h1> Estimating the Transition Probability Matrix in the <abbr title="Hidden Markov Model"> HMM</abbr>s.
        <small> A gentle Introduction to Markovian Processes and the Baum-Welch algorithm.  </small> </h1>
      <p>Author: Adrian Vrabie</p>
      <p>Adviser: Dr. habil. Univ Prof Lozovanu Dmitrii</p>

      <!-- The Kernel -->
      <div class="row">
         <div class="col-sm-2" style="background-color:lavender;">
           <h2>
             <small> The Kernel </small
           </h2>
         </div>
         <div class="col-sm-10" style="background-color:#dedede;">
           <h2> Motivation </h2>
           <p>
             The question that sparked my interest in studying Markovian processes is how to estimate the parameters of a HMM. In particular, given a set of observations, how to find the best estimates for the state transition stochastic matrix <span class="math inline">\(\mathbb{A}\)</span>.
           </p>
         </div>
       </div>
      <!-- Introduction-->
      <div class="row">
        <div class="col-sm-2" style="background-color:lavender;">
          <h2>  <small> Why study Markov Processes? </small></h2>
        </div>

        <div class="col-sm-10" style="background-color:#dedede;">
          <h2> Introduction </h2>
          <p> Markovian processes are ubiquitous in many real world applications, including algorithmic music composition, the Google search engine<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, asset pricing models, information processing, machine learning, computer malware detection<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> and many more.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> Markov chains can be used to help model how plants grow, chemicals react, and atoms diffuse and applications are increasingly being found in such areas as engineering, computer science, economics, and education. Jeffrey Kuan at Harvard University claimed that Markov chains not only had a tremendous influence on the development of mathematics, but that Markov models might well be <strong>the most <em>“real world”</em> useful mathematical concept after that of a derivative.</strong></p>
          <p> The Hidden Markov Model is the most suitable class among Markovian processes for modelling applications in Finance and Economics, yet the difficulties in estimating its parameters still present an issue for widespread adoption by the industry and academia.</p>
          <p> As we will see, Markovian chains and Hidden Markov Models have a rich yet accessible mathematical texture and have become increasingly applicable in a wide range of applications. Although the assumptions underpinning Markovian processes can be perceived as unacceptably restrictive at first, Markov models tend to fit the data particularly well. One can distinguish many types of Markovian processes, each with its set of particular features. In this tutorial we examine only one particular type: the models with time invariant probability distributions within a state.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> These models allow the use of the theoretical results from studies focused on the convergence properties of stationary distribution as time <span class="math inline">\(t \mapsto \infty\)</span>. This is of use in Economics because it opens avenues not only to reinterpret economic growth in the settings of a stochastic matrix but allows to efficiently compute expected long-term economic growth rates.</p>
          <p> The extension of Markovian chains to HMMs allows modelling even a wider scope of applications, suitable not only to describing the behaviour of the economy at a macroeconomic level but also for monetary policy advise. This can also have the potential of solving the Morgenstern’s critique.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> </p>
          <p> A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is the EM algorithm, see <span class="citation">A. P. Dempster (1977)</span>. The work of <span class="citation">A. P. Dempster (1977)</span> was based on the Ph.D. thesis of <span class="citation">Sundberg (1972)</span> which provided a very detailed treatment of the EM method for exponential functions. The first to describe this EM algorithm in the paradigm of a mathematical maximization technique for probabilistic functions in Markov chains was <span class="citation">Leonard E Baum et al. (1970)</span><a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a>. The paper of <span class="citation">(Rabiner 1989)</span> provided a practical guidance to understanding the results of <span class="citation">(Leonard E. Baum and Petrie 1966)</span> and <span class="citation">(Leonard E Baum et al. 1970)</span> and their application into an Engineering framework, specifically voice recognition tasks. In the same token, the papers <span class="citation">(James D Hamilton 2016)</span>, <span class="citation">(James D. Hamilton and Raj 2002)</span> and <span class="citation">(James D. Hamilton 2005)</span> adapted the mathematical techniques presented by <span class="citation">(Leonard E Baum et al. 1970)</span> in estimating the parameters for the regime-switching models in describing economic aggregates like growth rates. The same theoretical aspects discussed by <span class="citation">A. P. Dempster (1977)</span>, <span class="citation">Rabiner (1989)</span> and <span class="citation">Leonard E. Baum and Eagon (1967)</span> describe the Hidden Markov Models, moreover the EM algorithm is still the state of the art technique in estimating its parameters (<span class="math inline">\(\Theta\)</span>) for the underlying process of generating the observables which we denote by <span class="math inline">\(\mathcal{O}\)</span>. Ideally we would want to have a robust method of estimating the parameters of an HMM which performs well not only on past observations but also predict future outcomes. Such models could easily be adjusted to augment SDGE (Stochastic Dynamic General Equilibrium) models which are currently based on systems of difference equations. </p>
          <p> Unfortunately, there are still no analytical methods for estimating the transition probability that would guarantee the maximum of probabilities of a certain output generated by a Markovian process and we would still need to use a heuristic approach in determining the “right” number of states within a hidden Markov model. This is because any attempt to use any estimation methodologies suitable to the framework of Markovian processes undoubtedly inherits all its problems (for example the EM algorithm does not guarantee you a global minimum while the Clustering algorithms will not be able to determine a reasonable amount of focal points without an abstract cost function). Therefore, solving a problem with a hidden Markov chain requires a numerical approach.       </p>
          <p> The good news is that as computers become more powerful, not only more iterations are possible but also more attempts to find the maximum can be made. Paralel computer architectures with mapreduce functions allow even better optimization of the Baum-Welch algorithm and therefore a higher probability of finding the global maximum.  Nevertheless, a heuristic approach in chosing the model that makes sense after applying algorithms for estimating the transition probability matrix is, in my humble opinion, the most viable approach at the moment.</p>
        </div>

      </div>
      <!-- Markov Chains -->
      <div class="row">
        <div class="col-sm-2" style="background-color:lavender;">
          <h2>
             <small> What are Markovian Processes?</small>
          </h2>
        </div>
        <div class="col-sm-10" style="background-color:#dedede;">
          <h2> Markov Chains </h2>
          <blockquote>
            <p>
            One has to keep a particular openness of mind. Solving a problem is like going to a strange place, not to subdue it, but simply to spend time there, to preserve one’s openness, to wait for the signals, to wait for the strangeness to dissolve into sense.
            </p>
            <footer>Peter Whittle</footer>
          </blockquote>

          <p>A Markovian chain is a dynamical stochastic process which has the <em>Markovian property</em><a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a>. Before we formally introduce the notion of a <em>Markovian property</em>, it might be useful to take a step back and ask what a dynamical system is instead.</p>
          <p>Using the notation of <span class="citation">(Fraser 2008)</span>, a dynamical system is a mapping <span class="math inline">\(f(x_{t}) \mapsto \mathbb{R}^{n}\)</span>, where <span class="math inline">\(x_{t} \in \mathbb{R}^n\)</span> and <span class="math inline">\(t\)</span> is a time-like index, which transitions the state <span class="math inline">\(x_t\)</span> to <span class="math inline">\(x_{t+1}\)</span>.</p>
          <p>If this is also confusing perhaps the best way is to refer to real world examples: in Economics we might refer to <span class="math inline">\(x\)</span> as the “State of the Economy”, in tagging problems <span class="math inline">\(x\)</span> could be the part of speech in a sentence, in biology <span class="math inline">\(x\)</span> can be a tag from the set <span class="math inline">\(\{A,T,G,C\}\)</span> from the nucleotide sequences found in human DNA or <span class="math inline">\(x\)</span> could be a binary variable corresponding to whether a student gave a correct answer to a particular problem at the PISA test.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a>. In Economic models, since “the state of the economy” is an abstract term which encapsulates various positive and normative elements of the economy,<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a> we could restrict the values the economy can take to a particular set <span class="math inline">\(\mathbb{X} = \{\)</span> recession, mild- recession, mild-growth, growth <span class="math inline">\(\}\)</span>. The set <span class="math inline">\(X\)</span> is known as the <em>state space</em>.</p>
          <p>Given <span class="math inline">\(f(x)\)</span>, if <span class="math inline">\(x(t)\)</span> is known, one can deterministically find future values of <span class="math inline">\(x(t+1)\)</span>, <span class="math inline">\(x(t+2) \dots\)</span> independently of previous states <span class="math inline">\(x(t-1)\)</span>, <span class="math inline">\(x(t-2) \dots\)</span>, making historical information unnecessary. This “uselessness of history”, is also known as a <em>Markov property</em>. Statisticians might refer to the Markovian property by conditional independence of previous states given the current state. Therefore, a dynamical system is an instance of a Markov Chain since it satisfies the Markovian property.</p>
          <p>One implication is that such models are particularly appealing in models which emphasise fundamental analysis for determining the intrinsic value of financial assets.<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a></p>
          <!-- #redundant <p>Dynamic stochastic general equilibrium models (abbreviated DSGE or sometimes SDGE or DGE), used by the most influential central banks could also be augmented with Markovian processes. We could think of any dynamical systems and find ways to improve it with Markovian processes.</p> -->
          <p>Although Markov chains are useful in their own right, as we will show in section [smc], one problem we face in practice when the state <span class="math inline">\(x(t)\)</span> is latent<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a>. Usually we have lagged or only partial information about <span class="math inline">\(x(t)\)</span> and thus we can only estimate it. The information that is available to us, is called also called <em>emissions</em> in the literature, denoted by <span class="math inline">\(y(t)\)</span><a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a>. The observed variables are a function of the state the system is in, therefore we can represent: <span class="math display">\[y(t) \sim f(x(t))\]</span></p>
        </div>
      </div>

      <!-- Markov Chains and Applications in Economics -->
      <div class="row">
        <div class="col-sm-2" style="background-color:lavender;">
          <h2><small> Examples of Markovian Processes </small></h2>
        </div>
        <div class="col-sm-10" style="background-color:#dedede;">
          <h2 id="smc">Applications of Markov Chains in Economics </h2>
          <p>There are many types of Markov Chains, choosing the appropriate model depends on the specific problem being modelled:</p>
          <blockquote>
            <p>“ We may regard the present state of the universe as the effect of its past and the cause of its future ” </p>
            <footer> Marquis de Laplace </footer>
          </blockquote>
          <h3 id="first-order-markov-chain">First Order Markov Chain</h3>
            <p>Given a set <span class="math inline">\(\mathbf{X}\)</span> which forms the state space and <span class="math inline">\(\mathbf{\Sigma}\)</span> a <span class="math inline">\(\sigma\)</span>-algebra on <span class="math inline">\(\mathbf{X}\)</span> and a probability measure <span class="math inline">\(Pr\)</span>, a Markov Chain <span class="math inline">\(\{x_t\}\)</span> is a sequence of random variables with the property that the probability of moving from the present state <span class="math inline">\(x_t\)</span> to next state <span class="math inline">\(x_{t+1}\)</span> depends only on the present state.<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a> This property can be written as: <span class="math display">\[Pr\left(X_t = x_i |X_{t-1},X_{t-2},...,X_{1}  \right) = Pr\left( X_t=x_i|X_{t-1} \right)\]</span></p>
            <p>It is useful for abstraction purposes to represent a first order Markov process by a matrix <span class="math inline">\(\mathbb{A}\)</span> where the current state is represented by the row index. For this row to form a discrete probability distribution it must sum to 1. <span class="math display">\[\sum_{j \in X} a_{i,j} =1 \quad , \forall i  \in X\]</span></p>
            <p>Therefore, a first order Markov process is simply a reinterpretation of a probability transition matrix <span class="math inline">\(\mathbb{A}\)</span>, also called the stochastic matrix, where <span class="math inline">\(a_{ij} \in \mathbb{A}\)</span> represents the probability of observing outcome <span class="math inline">\(j\)</span> at <span class="math inline">\(t+1\)</span> if at time <span class="math inline">\(t\)</span> we are observing <span class="math inline">\(i\)</span>. In it’s simplest form, the future state depends only on the state we are currently in.<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a> We will deal only with time-homogeneous Markov chains in this paper.</p>
            <p>[Time Homogeneous Markov Chain] A time homogeneous Markov Chain is a MC that has a time invariant state-space probability matrix.</p>
            <p>Using the example from <span class="citation">James D. Hamilton (2005)</span><a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a> in his paper “What’s Real About the Business Cycle?”, we can write the state space transition matrix corresponding to the states <span class="math inline">\(\mathbf{X}= \{\)</span>normal growth, mild recession, severe recession <span class="math inline">\(\}\)</span> as:</p>
            <p><span class="math display">\[\mathbb{A} = \left( \begin{array}{ccc}
            0.971 &amp; 0.029 &amp; 0 \\
            0.145 &amp; 0.778 &amp; 0.077 \\
            0 &amp; 0.508 &amp; 0.492
            \end{array} \right)\]</span></p>
            <p>How is this useful in practice?<a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a> One example is to determine how many periods<a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a> of time a state will persist given that the current state <span class="math inline">\(X_t = x_t\)</span> as asked in the seminal paper of <span class="citation">(Rabiner 1989)</span>. One way to answer this question is to simulate the states generated by this matrix using Monte Carlo methods and then use the frequency approach to get an estimate. A better approach, shown in the paper of <span class="citation">Rabiner (1989)</span> is to observe that staying in a particular number of periods in a state follows a geometric series e.i. if we want to compute the probability the system will stay exactly 2 periods of time in the normal growth given that the current period <span class="math inline">\(x_t = \text{normal growth}\)</span> it will be: <span class="math display">\[\mathbb{P} = a_{11}^2(1-a_{11})\]</span> We know that the average value of <span class="math inline">\(x\)</span> denoted by <span class="math inline">\(\bar{x}\)</span> is <span class="math display">\[\bar{x} = \sum x \mathbb{P}(x)\]</span> similar to our case: the average (expected) number of days to stay in a particular state: <span class="math display">\[\bar{per} = \sum_{per=1}^\infty per a_{ii}^{d-1}(1-a_{ii})\]</span> which is the same as the well know geometric series<a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a>: <span class="math display">\[\sum_{k=1}^n k z^k = z\frac{1-(n+1)z^n+nz^{n+1}}{(1-z)^2}\]</span><a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a> Now taking <span class="math inline">\((1-a_{ii})\)</span> in front and taking the limit <span class="math inline">\(\lim_{t\to\infty} a_{ii}^t =0\)</span> we get <span class="math display">\[\bar{per} = (1-a_{ii}) \frac{1}{(1-a_{ii})^2} = \frac{1}{(1-a_{ii})}\]</span> So if we are in the state of <em>normal growth</em> at <span class="math inline">\(t\)</span> then we would expect: <span class="math display">\[\mathbb{E}[\text{periods of growth} | x_t = \text{normal growth}] = \frac{1}{1-0.971} \approx 34\]</span> This brings us back to the original question I posed about estimating a Markovian chain. If we know the expected number of periods a state persists in, we can calculate <span class="math inline">\(a_ii\)</span> from the stochastic matrix <span class="math inline">\(\mathbb{A}\)</span>.</p>
            <p>What is more useful in practice is that this matrix is of use when calculating the expected pay-off of a particular pro-cyclical investment project<a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a> or even better suited for financial assets. Let’s assume the following net pay-offs as a function of the state:</p>
            <p><span class="math display">\[\mathbb{E} = \left( \begin{array}{c}
            0.215 \\
            0.015 \\
            -0.18 \\
            \end{array} \right)\]</span></p>
            <p>That is, if the economy is in normal growth state, we expect the Internal Rate of Return to be 21.5 <span class="math inline">\(\%\)</span>.<a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a></p>
            <p>If the state <span class="math inline">\(x_t\)</span> is known at <span class="math inline">\(t\)</span> the expected pay-off is trivial: <span class="math display">\[E[t+1 | X_t= \texttt{mild recession}] = \sum_{j \in X} \mathbf{a}_{2,j}\mathbf{e}_j\]</span></p>
            <p>If the current state is not known, we can use a discrete distribution to assess in which state the economy is at time <span class="math inline">\(t\)</span>, following <span class="citation">(Lozovanu and Pickl 2015)</span> we will denote it by <span class="math inline">\(\mathbf{\mu}\)</span>. <span class="math display">\[\mu = \left( \begin{array}{c}
            0.1 \\
            0.55 \\
            0.35
            \end{array} \right)\]</span></p>
            <p>We can already calculate the expected pay-off using the Octave programming language.<a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a></p>
            <pre><code>A = [0.97100   0.02900   0.00000;
                 0.14500   0.77800   0.07700;
                 0.00000   0.50800   0.49200 ];
            E = [0.215,   0.015,  -0.18];
            E = transpose(E);
            mu = [0.1,   0.55,   0.35];

            #for the next period, given the state
            A*E</code></pre>
            <pre><code>ans =

               0.209200
               0.028985
              -0.080940</code></pre>
            <pre><code>#if the state is not known
            &gt;&gt; mu*A*E
            ans =  0.0085328</code></pre>
            <p>Given that the expected pay-off is basically zero, we might wrongly conclude that the project is not worth investing in.</p>
            <p>In petroleum industries however, as in the most other industries, the time horizon is not uncommon to be around 25 years. According to the signed Production Sharing Contracts (PSC) from the Kurdistan Region of Iraq published by the KRG Ministry of Natural Resources<a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a> we can freely examine a sample of 43 PSCs, the majority of them (41 out of 43) have a development period of 25 years (including the 5 years optional extension period).<a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a> Using this data, we can compute the expected pay-offs at time <span class="math inline">\(t=25\)</span> in the following way:</p>
            <p><span class="math display">\[E_{\textsf{Payoff}}[t=25] = \mu \mathbb{A}^{25} \mathbb{E}\]</span></p>
            <p>We can see that even in our example, computing <span class="math inline">\(\mathbb{A}^{25}\)</span> by hand is a very tedious task. And since matrix multiplication is very computationally intensive, computing matrices when <span class="math inline">\(t \mapsto \infty\)</span> becomes an issue.</p>
            <p>One way to solve this problem is to check if all the column vectors in <span class="math inline">\(\mathbb{A}\)</span> are independent. We can solve for the eigenvalues of <span class="math inline">\(\mathbb{A}\)</span> and if we have as many different eigenvalues <span class="math inline">\(n\)</span> as columns in <span class="math inline">\(\mathbb{A}\)</span> then we can find <span class="math inline">\(n\)</span> distinct eigenvectors and decompose <span class="math inline">\(\mathbb{A}\)</span> into its canonical form: <span class="math display">\[\label{eq:canonical}
            \mathbb{A} = \mathbf{S} \mathbf{\Lambda} \mathbf{S^{-1}}\]</span> where <span class="math inline">\(\mathbf{S}\)</span> is the matrix of eigenvectors and <span class="math inline">\(\mathbf{\Lambda}\)</span> is the identity matrix multiplied by the vector of eigenvalues. Now to solve for <span class="math display">\[E_{\textsf{Payoff}}[t=25] = \mu \mathbb{A}^{25} \mathbb{E} =  \mathbf{S} \mathbf{\Lambda}^{25} \mathbf{S^{-1}}\mathbb{E}\]</span> Solving for <span class="math inline">\(\mathbf{\Lambda}^{25}\)</span> is a lot easier than <span class="math inline">\(\mathbb{A}^{25}\)</span> since <span class="math inline">\(\mathbf{\Lambda}\)</span> is a diagonal matrix. Moreover, the highest eigenvalue of a the stochastic matrix generating a Markov chain is 1. This is important since if a matrix is: <span class="math display">\[\mathbf{x}\mathbb{A} = \lambda_i \mathbf{x}\]</span> then the stochastic matrix following a Markov chain can be written as:</p>
            <p><span class="math display">\[\mathbf{x}\mathbb{A} = \mathbf{x}\]</span></p>
            <p>which implies that <span class="math inline">\(\mathbf{x}\)</span> is a stationary probability distribution.<a href="#fn25" class="footnoteRef" id="fnref25"><sup>25</sup></a> To find the eigenvalues of <span class="math inline">\(\mathbb{A}\)</span> we proceed as follows: <span class="math display">\[\mathbf{v}(\mathbb{A}-I\lambda ) = \mathbf{0}\]</span> where <span class="math inline">\(\mathbf{v}\)</span> is one of the non-trivial eigenvectors<a href="#fn26" class="footnoteRef" id="fnref26"><sup>26</sup></a>. Determining the set of all <span class="math inline">\(\lambda\)</span>s for which the determinant of <span class="math inline">\(\mathbb{A}-I\lambda \)</span> in the above equation is zero is simply solving an <span class="math inline">\(n^{\texttt{th}}\)</span> order polynomial which I would rather do in Octave as follows:</p>
            <p><span class="math display">\[\left| \mathbb{A}-I\lambda \right| = \left| \begin{array}{ccc}
            0.971-\lambda &amp; 0.029 &amp; 0 \\
            0.145 &amp; 0.778-\lambda &amp; 0.077 \\
            0 &amp; 0.508 &amp; 0.492-\lambda
            \end{array} \right| =0\]</span></p>
            <pre><code>&gt;&gt; eigs(A)
            ans =
               1.00000
               0.85157
               0.38943</code></pre>
            <p>That is: <span class="math display">\[\mathbf{\lambda} = \left(\begin{array}{c}
               1.00000 \\
               0.85157 \\
               0.38943 \\
            \end{array} \right)\]</span> We can verify that these are indeed the eigenvalues of <span class="math inline">\(\mathbb{A}\)</span> by comparing the trace of <span class="math inline">\(\mathbb{A}\)</span> with the sum of the eigenvalues and the determinant of <span class="math inline">\(\mathbb{A}\)</span> should be equal to the product of the eigenvalues.<a href="#fn27" class="footnoteRef" id="fnref27"><sup>27</sup></a></p>
            <pre><code>&gt;&gt; trace(A)
            ans =  2.2410
            &gt;&gt; sum(eig(A))
            ans =  2.2410
            &gt;&gt; prod(eig(A))
            ans =  0.33163
            &gt;&gt; det(A)
            ans =  0.33163</code></pre>
            <p>Now having found the eigenvalues, we substitute each of the eigenvalues into <span class="math inline">\(\lambda\)</span> and get a 3 degenerate matrices. We can easily verify this:</p>
            <pre><code>&gt;&gt; det(A-eye(3))
            ans =   -1.6611e-18</code></pre>
            <p>which we assume is <span class="math inline">\(0\)</span> due to rounding errors.</p>
            <p>Then we find the null space of these new matrices and find the eigenvectors corresponding to each eigenvalue.</p>
            <pre><code>&gt;&gt; [evects, evals] = eigs(A)
            evects =

               0.5773503  -0.1389312   0.0098689
               0.5773503   0.5721371  -0.1979135
               0.5773503   0.8083052   0.9801698

            evals =

            Diagonal Matrix

               1.00000         0         0
                     0   0.85157         0
                     0         0   0.38943</code></pre>
            <p>Therefore, since <span class="math inline">\(\mathbf{S}\)</span> is: <span class="math display">\[\mathbf{S} = \left(\begin{array}{ccc}
               0.5773503 &amp;-0.1389312 &amp; 0.0098689 \\
               0.5773503 &amp; 0.5721371 &amp;-0.1979135 \\
               0.5773503 &amp; 0.8083052 &amp; 0.9801698 \\

            \end{array}\right)\]</span> then <span class="math inline">\(\mathbf{S}^{-1}\)</span> is:</p>
            <pre><code>&gt;&gt; pinv(evects)
            ans =</code></pre>
            <p><span class="math display">\[\mathbf{S}^{-1} = \left(\begin{array}{ccc}
            1.407811   &amp;0.281562   &amp;0.042678 \\
            -1.328512   &amp;1.094198   &amp;0.234314 \\
            0.266324  &amp;-1.068188   &amp;0.801864 \\
            \end{array}\right)\]</span></p>
            <p>Now we are able to compute the values of the vector from equation [eq:canonical] :</p>
            <pre><code>&gt;&gt; evects*(evals^25)*pinv(evects)
            ans =</code></pre>
            <p><span class="math display">\[\mathbf{S}\mathbf{\Lambda}^{25}\mathbf{S}^{-1} = \left(\begin{array}{ccc}
               0.816125   &amp;0.159822   &amp;0.024054 \\
               0.799109   &amp;0.173836   &amp;0.027055 \\
               0.793458   &amp;0.178491   &amp;0.028051 \\
            \end{array}\right)\]</span></p>
            <p>What if <span class="math inline">\(\mathbb{A}\)</span> is not irreducible<a href="#fn28" class="footnoteRef" id="fnref28"><sup>28</sup></a> like in a for of a diagonal matrix? In this case, even if we get <span class="math inline">\(n\)</span> different eigenvalues we will not get <span class="math inline">\(n\)</span> orthogonal<a href="#fn29" class="footnoteRef" id="fnref29"><sup>29</sup></a> eigenvectors. To solve these types of problems we can use the algorithms proposed by <span class="citation">(Lozovanu and Pickl 2015)</span> for solving for <span class="math inline">\(\mathbb{Q}\)</span><a href="#fn30" class="footnoteRef" id="fnref30"><sup>30</sup></a> which can solve it in <span class="math inline">\(\mathcal{O}(n^3)\)</span> operations.</p>
            <p>If eigenvectors seem too foreign or the method is too confusing and since we only have 3 states and <span class="math inline">\(t=25\)</span> we can directly solve it using any programming language. In Octave or Matlab it is as simple as:</p>
            <pre><code>&gt;&gt; mu*A^25*E
            ans =  0.16948</code></pre>
            <p>Now the expected return is 17<span class="math inline">\(\%\)</span> and the decision for Marathon to invest in developing this region will depend on on their ability to attract capital with less than 17<span class="math inline">\(\%\)</span> interest as well as the availability of other more attractive opportunities.<a href="#fn31" class="footnoteRef" id="fnref31"><sup>31</sup></a></p>
            <p>Another curiosity is the range the IRR takes as a function of <span class="math inline">\(\mu\)</span>. Again, with a Markovian chain this is trivial once we have our limiting probability matrix <span class="math inline">\(\mathbb{Q}\)</span> or <span class="math inline">\(\mathbb{A}^t\)</span>. Using the transition matrix provided by <span class="citation">James D. Hamilton (2005)</span>, the long term expectation of being in a particular state can be written as: <span class="math display">\[E[X] = \mu \mathbb{Q}\]</span> Even if we take <span class="math inline">\(\mu\)</span> to be <span class="math inline">\([0,0,1]\)</span> e.i. the worst case scenario:</p>
            <pre><code>&gt;&gt; mu
            mu =
               0   0   1
            &gt;&gt; mu*A^25
            ans =
               0.793458   0.178491   0.028051
            &gt;&gt; mu*A^25*E
            ans =  0.16822</code></pre>
            <p>we can still expect a 16.8<span class="math inline">\(\%\)</span> return. To understand why this is so, we have to introduce the Fundamental theorem of Markov Chains.</p>


        </div>

      </div>

      <!-- References and Footnotes -->
      <h2>   References  </h2>
        <div id="refs" class="references">
          <div id="ref-Dempster77">
          <p>A. P. Dempster, D. B. Rubin, N. M. Laird. 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm.” <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> 39 (1). [Royal Statistical Society, Wiley]: 1–38. <a href="http://www.jstor.org/stable/2984875" class="uri">http://www.jstor.org/stable/2984875</a>.</p>
          </div>
          <div id="ref-barbu08">
          <p>Barbu, V.S., and N. Limnios. 2008. <em>Semi-Markov Chains and Hidden Semi-Markov Models Toward Applications: Their Use in Reliability and DNA Analysis</em>. Lecture Notes in Statistics. Springer New York. <a href="https://books.google.md/books?id=aNPajwEACAAJ" class="uri">https://books.google.md/books?id=aNPajwEACAAJ</a>.</p>
          </div>
          <div id="ref-baum1970maximization">
          <p>Baum, Leonard E, Ted Petrie, George Soules, and Norman Weiss. 1970. “A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains.” <em>The Annals of Mathematical Statistics</em> 41 (1). JSTOR: 164–71.</p>
          </div>
          <div id="ref-baum1967">
          <p>Baum, Leonard E., and J. A. Eagon. 1967. “An Inequality with Applications to Statistical Estimation for Probabilistic Functions of Markov Processes and to a Model for Ecology.” <em>Bull. Amer. Math. Soc.</em> 73 (3). American Mathematical Society: 360–63. <a href="http://projecteuclid.org/euclid.bams/1183528841" class="uri">http://projecteuclid.org/euclid.bams/1183528841</a>.</p>
          </div>
          <div id="ref-baum66">
          <p>Baum, Leonard E., and Ted Petrie. 1966. “Statistical Inference for Probabilistic Functions of Finite State Markov Chains.” <em>Ann. Math. Statist.</em> 37 (6). The Institute of Mathematical Statistics: 1554–63. doi:<a href="https://doi.org/10.1214/aoms/1177699147">10.1214/aoms/1177699147</a>.</p>
          </div>
          <div id="ref-benesch2001baum">
          <p>Benesch, T. 2001. “The Baum–Welch Algorithm for Parameter Estimation of Gaussian Autoregressive Mixture Models.” <em>Journal of Mathematical Sciences</em> 105 (6). Springer: 2515–8.</p>
          </div>
          <div id="ref-fraser08">
          <p>Fraser, A.M. 2008. <em>Hidden Markov Models and Dynamical Systems</em>. SIAM E-Books. Society for Industrial; Applied Mathematics (SIAM, 3600 Market Street, Floor 6, Philadelphia, PA 19104). <a href="https://books.google.md/books?id=DMEWrB-2gGYC" class="uri">https://books.google.md/books?id=DMEWrB-2gGYC</a>.</p>
          </div>
          <div id="ref-Haggstrom02">
          <p>Häggström, Olle. 2002. <em>Finite Markov Chains and Algorithmic Applications (London Mathematical Society Student Texts)</em>. 1st ed. Cambridge University Press. <a href="http://amazon.com/o/ASIN/0521890012/" class="uri">http://amazon.com/o/ASIN/0521890012/</a>.</p>
          </div>
          <div id="ref-hamilton2016macroeconomic">
          <p>Hamilton, James D. 2016. “Macroeconomic Regimes and Regime Shifts.” National Bureau of Economic Research.</p>
          </div>
          <div id="ref-hamilton05">
          <p>Hamilton, James D. 2005. “What’s Real About the Business Cycle?” Working Paper, Working paper series, no. 11161 (February). National Bureau of Economic Research. doi:<a href="https://doi.org/10.3386/w11161">10.3386/w11161</a>.</p>
          </div>
          <div id="ref-hamilton02">
          <p>Hamilton, James D., and Baldev Raj. 2002. <em>Advances in Markov-Switching Models</em>. Springer Science Business Media. doi:<a href="https://doi.org/10.1007/978-3-642-51182-0">10.1007/978-3-642-51182-0</a>.</p>
          </div>
          <div id="ref-langville2011google">
          <p>Langville, Amy N, and Carl D Meyer. 2011. <em>Google’s PageRank and Beyond: The Science of Search Engine Rankings</em>. Princeton University Press.</p>
          </div>
          <div id="ref-lin2011hunting">
          <p>Lin, Da, and Mark Stamp. 2011. “Hunting for Undetectable Metamorphic Viruses.” <em>Journal in Computer Virology</em> 7 (3). Springer: 201–14.</p>
          </div>
          <div id="ref-Lozovanu15">
          <p>Lozovanu, Dmitrii, and Stefan Pickl. 2015. <em>Optimization of Stochastic Discrete Systems and Control on Complex Networks</em>. Springer International Publishing. doi:<a href="https://doi.org/10.1007/978-3-319-11833-8">10.1007/978-3-319-11833-8</a>.</p>
          </div>
          <div id="ref-Rabiner89">
          <p>Rabiner, L. R. 1989. “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.” <em>Proceedings of the IEEE</em> 77 (2): 257–86. doi:<a href="https://doi.org/10.1109/5.18626">10.1109/5.18626</a>.</p>
          </div>
          <div id="ref-Romer06">
          <p>Romer, D. 2006. <em>Advanced Macroeconomics</em>. McGraw-Hill Higher Education. McGraw-Hill. <a href="https://books.google.md/books?id=9dW7AAAAIAAJ" class="uri">https://books.google.md/books?id=9dW7AAAAIAAJ</a>.</p>
          </div>
          <div id="ref-Stachurski2008">
          <p>Stachurski, John, and Vance Martin. 2008. “Computing the Distributions of Economic Models via Simulation.” <em>Econometrica</em> 76 (2). The Econometric Society: 443–50. doi:<a href="https://doi.org/10.1111/j.1468-0262.2008.00839.x">10.1111/j.1468-0262.2008.00839.x</a>.</p>
          </div>
          <div id="ref-quantecon">
          <p>Stachurski, John, and Thomas J. Sargent. 2016. “Quant Econ Quantative Economics.” <a href="http://quant-econ.net/" class="uri">http://quant-econ.net/</a>.</p>
          </div>
          <div id="ref-MITLA">
          <p>Strang, Gilbert. 2011. “18.06SC Linear Algebra.” Massachusetts Institute of Technology: MIT OpenCourseWare. <a href="http://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/index.htm" class="uri">http://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/index.htm</a>.</p>
          </div>
          <div id="ref-sundberg1972maximum">
          <p>Sundberg, R. 1972. “Maximum Likelihood Theory and Applications for Distributions Generated When Observing a Function of an Exponential Variable.” PhD thesis, PhD Thesis. Institute of Mathematics; Statistics, Stockholm University, Stockholm.</p>
          </div>
          <div id="ref-Tauchen86">
          <p>Tauchen, George. 1986. “Finite State Markov-Chain Approximations to Univariate and Vector Autoregressions.” <em>Economics Letters</em> 20 (2). Elsevier BV: 177–81. doi:<a href="https://doi.org/10.1016/0165-1765(86)90168-0">10.1016/0165-1765(86)90168-0</a>.</p>
          </div>
        </div>
        <div class="footnotes">
          <hr />
          <ol>
          <li id="fn1"><p>see: <span class="citation">Langville and Meyer (2011)</span> and <span class="citation">Stachurski and Martin (2008)</span>,<a href="#fnref1">↩</a></p></li>
          <li id="fn2"><p>For computer viruses and malware detection using Hidden Markov Models, see <span class="citation">(Lin and Stamp 2011)</span><a href="#fnref2">↩</a></p></li>
          <li id="fn3"><p>see speech: <a href="http://www.math.harvard.edu/~kmatveev/markov.html">http://www.math.harvard.edu/ kmatveev/markov.html</a><a href="#fnref3">↩</a></p></li>
          <li id="fn4"><p>Analogous to difference equations, we are not interested in the systems dependent on time <span class="math inline">\(t\)</span> as this will exponentially complicate our estimation technique.<a href="#fnref4">↩</a></p></li>
          <li id="fn5"><p>In his book, On the Accuracy of Economic Observations (1950), Morgenstern expressed his concerns in the way the data is used from the national income accounts to reach conclusions about the state of the economy and about appropriate policies. Note for Mathematicians: Morgenstern was a friend of John von Neumann.<a href="#fnref5">↩</a></p></li>
          <li id="fn6"><p>This is probably the reason this adaptation of the EM algorithm is called the Baum-Welch algorithm.<a href="#fnref6">↩</a></p></li>
          <li id="fn7"><p>We define formally a Markov chain in section [smc]<a href="#fnref7">↩</a></p></li>
          <li id="fn8"><p>For example, at the Math PISA test <span class="math inline">\(x_{ij}\)</span> could be whether student <span class="math inline">\(i\)</span> answered correctly problem <span class="math inline">\(j\)</span>.<a href="#fnref8">↩</a></p></li>
          <li id="fn9"><p>The National Bureau of Economic Research (NBER) defines recession as “a significant decline in economic activity spread across the economy, lasting more than a few months, normally visible in real GDP, real income, employment, industrial production, and wholesale-retail sales.”, this is a much broader view than simply a decrease in GDP.<a href="#fnref9">↩</a></p></li>
          <li id="fn10"><p>For example, the Gordon Dividend Discount Model which is augmented with a stream of dividends that are governed by a state transition matrix or a HMM which we will present in section [HMM].<a href="#fnref10">↩</a></p></li>
          <li id="fn11"><p>which is a fancy way of saying that variables of interest are not always directly observable. Example: Suppose you’re looking for a partner and you want it to be intelligent. The IQ however is not directly observable, and you would have to infer it using his or her behaviour as a function of IQ.<a href="#fnref11">↩</a></p></li>
          <li id="fn12"><p>Which is a fancy way of saying “observations”. The reason for that can be traced back to the applications of the Markovian processes in speech recognition tasks.<a href="#fnref12">↩</a></p></li>
          <li id="fn13"><p>We could also think of an zero-order Markov process, the case when the current state is completely independent of the previous state, like throwing a dice. But then we simply get back to a classical probability distribution. If <span class="math inline">\(\left(\mathbf{\Omega, \Sigma, Pr}\right)\)</span> is a discrete sample space where <span class="math inline">\(\mathbf{\Omega}\)</span> is the set of all the possible outcomes, <span class="math inline">\(Pr: \Sigma \mapsto \mathbb{R}\)</span> where <span class="math inline">\(\sum_{x_{i}\in\mathbf{\Omega}}{Pr(x_{i})} =1\)</span>. In this case: <span class="math inline">\(Pr\left(X_t = x_i |X_{t-1},X_{t-2},...,X_{1}  \right) = Pr\left( X_t=x_i \right)\)</span>, therefore, it is completely redundant to introduce a zero-order Markov processes.<a href="#fnref13">↩</a></p></li>
          <li id="fn14"><p>Please note that in a simple Markov Chain, unlike a Hidden Markov Model which we will define later, the states are observable.<a href="#fnref14">↩</a></p></li>
          <li id="fn15"><p>One of the key insights of this paper is that a linear statistical model with homoskedastic errors cannot capture the nineteenth-century notion of a recurring cyclical pattern in key economic aggregates and that a simple Markov chain has a much better goodness of fit.<a href="#fnref15">↩</a></p></li>
          <li id="fn16"><p>You can also simulate a Markov Chain given a stochastic matrix at <a href="http://setosa.io/ev/markov-chains/" class="uri">http://setosa.io/ev/markov-chains/</a><a href="#fnref16">↩</a></p></li>
          <li id="fn17"><p>period is an abstract term, in the paper of <span class="citation">Rabiner (1989)</span> days are assumed, in the example of <span class="citation">James D. Hamilton and Raj (2002)</span> months, however for economic aggregates usually quarters are assumed<a href="#fnref17">↩</a></p></li>
          <li id="fn18"><p>according to a Math professor from MIT (quote needed), this is the second most beautiful series in Math after <span class="math inline">\(e^x\)</span><a href="#fnref18">↩</a></p></li>
          <li id="fn19"><p>other beautiful series derived from the geometric series: <a href="http://lycofs01.lycoming.edu/~sprgene/M332/Sums_Series.pdf"> http://lycofs01.lycoming.edu/ sprgene/M332/Sums_Series.pdf</a><a href="#fnref19">↩</a></p></li>
          <li id="fn20"><p>data from the upstream oil industry as an example<a href="#fnref20">↩</a></p></li>
          <li id="fn21"><p>In practice it is easier to estimate the profits of a given project in a year given the state of the economy.<a href="#fnref21">↩</a></p></li>
          <li id="fn22"><p>Octave programming language is very similar to Matlab, except that it is free and open source.<a href="#fnref22">↩</a></p></li>
          <li id="fn23"><p>The Production Sharing Contracts can be found at <a href="http://cabinet.gov.krd/p/p.aspx?l=12&amp;p=1" class="uri">http://cabinet.gov.krd/p/p.aspx?l=12&amp;p=1</a><a href="#fnref23">↩</a></p></li>
          <li id="fn24"><p>This can be usually found at Article 6 clause 6.9, 6.10, 6.11 and 6.12, for example the Contract signed between Marathon and KRG at <a href="http://cabinet.gov.krd/p/p.aspx?l=12&amp;r=296&amp;h=1&amp;s=030000&amp;p=70" class="uri">http://cabinet.gov.krd/p/p.aspx?l=12&amp;r=296&amp;h=1&amp;s=030000&amp;p=70</a><a href="#fnref24">↩</a></p></li>
          <li id="fn25"><p><span class="math inline">\(\mathbf{x}\mathbb{A} = \mathbf{x}\)</span> should not be confused with <span class="math inline">\(\mathbb{A}\mathbf{x} = \mathbf{x}\)</span> since any vector <span class="math inline">\(\mathbf{x}\)</span> which has <span class="math inline">\(x_i = x_j\)</span> satisfies this equality.<a href="#fnref25">↩</a></p></li>
          <li id="fn26"><p>that is <span class="math inline">\(\mathbf{v}\)</span> is not a zero vector<a href="#fnref26">↩</a></p></li>
          <li id="fn27"><p>In Octave as well as in other programming languages directly comparing sum(eig(A)) == trace(A) will usually not work due to rounding errors.<a href="#fnref27">↩</a></p></li>
          <li id="fn28"><p>we will define what irreducible is later.<a href="#fnref28">↩</a></p></li>
          <li id="fn29"><p>orthogonal is just another way of saying perpendicular or independent vectors<a href="#fnref29">↩</a></p></li>
          <li id="fn30"><p>the limiting probability matrix is denoted by <span class="math inline">\(\mathbb{Q}\)</span><a href="#fnref30">↩</a></p></li>
          <li id="fn31"><p>Of course we are assuming that the space state transition probability matrix <span class="math inline">\(\mathbb{A}\)</span> computed by <span class="citation">James D. Hamilton (2005)</span> for the United States is valid for Kurdistan Region of Iraq. For these conclusions to have any validity, the study of Hamilton needs to be replicated for KRG and more than 3 states would be desirable to be used.<a href="#fnref31">↩</a></p></li>
          <li id="fn32"><p>we will present a model when the discount rate is not zero<a href="#fnref32">↩</a></p></li>
          <li id="fn33"><p>also called stochastic kernel in literature, see for example http://quant-econ.net/jl/stationary_densities.html<a href="#fnref33">↩</a></p></li>
          <li id="fn34"><p>In a nutshell, the Solow–Swan model assumes a closed market economy. A single good (output) is produced using two factors of production, labour <span class="math inline">\(L\)</span> and capital <span class="math inline">\(K\)</span> in an aggregate production function that satisfies the Inada conditions, which imply that the elasticity of substitution must be asymptotically equal to one. https://en.wikipedia.org/wiki/Solow-Swan_model<a href="#fnref34">↩</a></p></li>
          <li id="fn35"><p>Robert Solow has was awarder the Nobel Prize in Economics.<a href="#fnref35">↩</a></p></li>
          <li id="fn36"><p>For example the open source library KernelDensity for Julia programming language which is hosted at https://github.com/JuliaStats/KernelDensity.jl<a href="#fnref36">↩</a></p></li>
          <li id="fn37"><p>For a 2 state economy over a span of 10 years, we would call our function 1024 times, in contrast, a 3 state economy on quarterly data would require 12157665459056928801 (<span class="math inline">\(1.22*10^{19}\)</span>) calls.<a href="#fnref37">↩</a></p></li>
          <li id="fn38"><p>usually denoted by <span class="math inline">\(\varepsilon_i(k)\)</span> in the literature<a href="#fnref38">↩</a></p></li>
          <li id="fn39"><p>In the appendix I will present the Bayesian rules in probability<a href="#fnref39">↩</a></p></li>
          <li id="fn40"><p>Since the current outcome depends on the current state and not on past outcomes <span class="math inline">\(Pr(o_k|x_k,o_{k-1},o_{k-2}...) =Pr(o_k|x_k) \)</span>. In practice, it is reasonable to assume that the expected outcome of a function (the growth of Economy) depends on the intrinsic values that define the system, rather than past outcomes.<a href="#fnref40">↩</a></p></li>
          <li id="fn41"><p>Mathematicalmonk channel present a gentle introduction to HMM and builds intuition what types of questions we can answer https://www.youtube.com/watch?v=7zDARfKVm7s&amp;list=PLD0F06AA0D2E8FFBA<a href="#fnref41">↩</a></p></li>
          <li id="fn42"><p>usually denoted by <span class="math inline">\(\varepsilon_i(k)\)</span> in the literature<a href="#fnref42">↩</a></p></li>
          <li id="fn43"><p>I follow the notation of: http://personal.ee.surrey.ac.uk/Personal/P.Jackson/tutorial/hmm_tut2.pdf<a href="#fnref43">↩</a></p></li>
          <li id="fn44"><p>I used the steps of the algorithm from Wikipedia, https://en.wikipedia.org/wiki/Forward_algorithm except that I corrected for mistakes. For example: Wiki says it uses chain rule, when they meant the law of total probability.<a href="#fnref44">↩</a></p></li>
          <li id="fn45"><p>Simple as it may sound, according to David Forney<a href="#fnref45">↩</a></p></li>
          <li id="fn46"><p>https://en.wikipedia.org/wiki/Baum-Welch_algorithm<a href="#fnref46">↩</a></p></li>
          <li id="fn47"><p>see: http://ghmm.org/<a href="#fnref47">↩</a></p></li>
          <li id="fn48"><p>link: <a href="http://dna.cs.byu.edu/bio465/Labs/hmmtut.shtml" class="uri">http://dna.cs.byu.edu/bio465/Labs/hmmtut.shtml</a><a href="#fnref48">↩</a></p></li>
          <li id="fn49"><p><a href="https://github.com/moldovean/usm/tree/master/Thesis" class="uri">https://github.com/moldovean/usm/tree/master/Thesis</a><a href="#fnref49">↩</a></p></li>
          <li id="fn50"><p>link: <a href="http://sourceforge.net/svn/?group_id=67094" class="uri">http://sourceforge.net/svn/?group_id=67094</a><a href="#fnref50">↩</a></p></li>
          <li id="fn51"><p>link: http://stats.stackexchange.com/questions/70545/looking-for-a-good-and-complete-probability-and-statistics-book<a href="#fnref51">↩</a></p></li>
          <li id="fn52"><p>link: <a href="https://onlinecourses.science.psu.edu/stat414/node/241" class="uri">https://onlinecourses.science.psu.edu/stat414/node/241</a><a href="#fnref52">↩</a></p></li>
          <li id="fn53"><p>link: https://www.statlect.com/fundamentals-of-probability/Bayes-rule<a href="#fnref53">↩</a></p></li>
          <li id="fn54"><p>link: http://stattrek.com/probability/bayes-theorem.aspx<a href="#fnref54">↩</a></p></li>
          <li id="fn55"><p>link: http://setosa.io/ev/eigenvectors-and-eigenvalues/<a href="#fnref55">↩</a></p></li>
          <li id="fn56"><p><a href="http://ghmm.org" class="uri">http://ghmm.org</a><a href="#fnref56">↩</a></p></li>
          <li id="fn57"><p><a href="http://www.cs.rutgers.edu/~schliep/index.html">http://www.cs.rutgers.edu/ schliep/index.html</a><a href="#fnref57">↩</a></p></li>
          <li id="fn58"><p><a href="https://en.wikipedia.org/wiki/Andrey_Markov" class="uri">https://en.wikipedia.org/wiki/Andrey_Markov</a><a href="#fnref58">↩</a></p></li>
          </ol>
          </div>
    </div>

  </body>
</html>
